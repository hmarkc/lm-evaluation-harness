{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cnn_dailymail': [0.43180575314268377, 0.43158987392961773, 0.4165613655782313, 0.4155405794753616], 'mmlu': [1.0067294751009421, 1.003460872909056, 1.003460872909056, 1.0032686021918862], 'piqa': [1.0343007915567282, 1.0343007915567282, 1.0343007915567282, 1.0343007915567282], 'wmt16-ro-en': [1.5152866771017584, 1.513853890711738, 1.5141694691005363, 1.5130880444042472]}\n",
      "{'cnn_dailymail': [0.4335744366634509, 0.43002517920858696, 0.4335744366634509, 0.421943356621873], 'mmlu': [1.004999038646414, 1.0051913093635838, 1.004999038646414, 1.0030763314747162], 'piqa': [1.0343007915567282, 1.0343007915567282, 1.0343007915567282, 1.0343007915567282], 'wmt16-ro-en': [1.5148598674999743, 1.5177421423673756, 1.5148598674999743, 1.5133908796773319]}\n",
      "0.9970306742255282 0\n",
      "0.9969335335916418 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "file = \"results/__home__gstmchen__optimized-model-merging__generative__outs__llama_merged__frank_wolfe__frank_wolfe_merge2_01/results_2025-02-07T11-34-16.745375.json\"\n",
    "tie_prefix = \"__home__gstmchen__optimized-model-merging__generative__outs__llama_merged__frank_wolfe__frank_wolfe_merge_\"\n",
    "task_arith_prefix = \"__home__gstmchen__optimized-model-merging__generative__outs__llama_merged__frank_wolfe__frank_wolfe_merge2_\"\n",
    "\n",
    "baseline_scores = {\n",
    "    'cnn_dailymail': 0.12675733665329772, \n",
    "    'mmlu': 0.4272570442783209, \n",
    "    'piqa': 0.758, \n",
    "    'wmt16-ro-en': 18.599623556677553}\n",
    "\n",
    "tie_dict = {\n",
    "    'cnn_dailymail': [], \n",
    "    'mmlu': [],\n",
    "    'piqa': [],\n",
    "    'wmt16-ro-en': [],\n",
    "}\n",
    "task_arith_dict = {\n",
    "    'cnn_dailymail': [],\n",
    "    'mmlu': [],\n",
    "    'piqa': [],\n",
    "    'wmt16-ro-en': [],\n",
    "}\n",
    "\n",
    "metric_map = {\n",
    "    'cnn_dailymail': 'rouge,none',\n",
    "    'mmlu': 'acc,none',\n",
    "    'piqa': 'acc,none',\n",
    "    'wmt16-ro-en': 'bleu,none',\n",
    "}\n",
    "\n",
    "for folder_name in os.listdir(\"results\"):\n",
    "    file_name = sorted(f for f in os.listdir(\"results/\" + folder_name) if f.startswith(\"results_\"))[-1]\n",
    "    with open(\"results/\" + folder_name + \"/\" + file_name, 'r') as f:\n",
    "        results = json.load(f)['results']\n",
    "    for key in results.keys():\n",
    "        if key not in metric_map:\n",
    "            continue\n",
    "        d = results[key][metric_map[key]] / baseline_scores[key]\n",
    "        if folder_name.startswith(tie_prefix):\n",
    "            tie_dict[key].append(d)\n",
    "        elif folder_name.startswith(task_arith_prefix):\n",
    "            task_arith_dict[key].append(d)\n",
    "\n",
    "print(tie_dict)\n",
    "print(task_arith_dict)\n",
    "\n",
    "max_tie_score = 0 \n",
    "max_tie_key = None\n",
    "max_task_arith_score = 0\n",
    "max_task_arith_key = None\n",
    "task_num = len(tie_dict['cnn_dailymail'])\n",
    "for i in range(task_num):\n",
    "    tie_score = sum(tie_dict[key][i] for key in tie_dict.keys()) / task_num\n",
    "    task_arith_score = sum(task_arith_dict[key][i] for key in task_arith_dict.keys()) / task_num\n",
    "    if tie_score > max_tie_score:\n",
    "        max_tie_score = tie_score\n",
    "        max_tie_key = i\n",
    "    if task_arith_score > max_task_arith_score:\n",
    "        max_task_arith_score = task_arith_score\n",
    "        max_task_arith_key = i\n",
    "\n",
    "print(max_tie_score, max_tie_key)\n",
    "print(max_task_arith_score, max_task_arith_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cnn_dailymail': [0.12675733665329772], 'mmlu': [0.4272570442783209], 'piqa': [0.758], 'wmt16-ro-en': [18.599623556677553]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "file = \"results_baseline/cnn/__home__gstmchen__optimized-model-merging__llama__llama-cnn__merged/results_2025-02-07T08-21-24.606050.json\"\n",
    "\n",
    "baseline_dict = {\n",
    "    'cnn_dailymail': [],\n",
    "    'mmlu': [],\n",
    "    'piqa': [],\n",
    "    'wmt16-ro-en': [],\n",
    "}\n",
    "\n",
    "metric_map = {\n",
    "    'cnn_dailymail': 'rouge,none',\n",
    "    'mmlu': 'acc,none',\n",
    "    'piqa': 'acc,none',\n",
    "    'wmt16-ro-en': 'bleu,none',\n",
    "}\n",
    "\n",
    "for folder_name1 in os.listdir(\"results_baseline\"):\n",
    "  for folder_name2 in os.listdir(\"results_baseline/\" + folder_name1):\n",
    "    with open(f\"results_baseline/{folder_name1}/{folder_name2}/{sorted(f for f in os.listdir(f'results_baseline/{folder_name1}/{folder_name2}') if f.startswith('results_'))[-1]}\", 'r') as f:\n",
    "        results = json.load(f)['results']\n",
    "    for key, value in results.items():\n",
    "        if key in metric_map:\n",
    "            baseline_dict[key].append(value[metric_map[key]])\n",
    "\n",
    "print(baseline_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gstmchen/miniconda3/envs/lm-eval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-07 00:33:45 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 00:33:45,799\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-07 00:33:46 config.py:2364] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-07 00:33:54 config.py:526] This model supports multiple tasks: {'generate', 'classify', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 02-07 00:33:54 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='../optimized-model-merging/llama/llama-mmlu/merged', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=../optimized-model-merging/llama/llama-mmlu/merged, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-07 00:33:56 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 02-07 00:33:56 model_runner.py:1111] Starting to load model ../optimized-model-merging/llama/llama-mmlu/merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W207 00:33:56.412516038 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [ws5]:53411 (errno: 97 - Address family not supported by protocol).\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:05,  1.08s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:02<00:04,  1.25s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:03<00:03,  1.29s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:05<00:02,  1.33s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:06<00:01,  1.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:07<00:00,  1.21s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:07<00:00,  1.25s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-07 00:34:05 model_runner.py:1116] Loading model weights took 12.5523 GB\n",
      "INFO 02-07 00:34:07 worker.py:266] Memory profiling takes 1.72 seconds\n",
      "INFO 02-07 00:34:07 worker.py:266] the current vLLM instance can use total_gpu_memory (39.50GiB) x gpu_memory_utilization (0.50) = 19.75GiB\n",
      "INFO 02-07 00:34:07 worker.py:266] model weights take 12.55GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 6.75GiB.\n",
      "INFO 02-07 00:34:07 executor_base.py:108] # CUDA blocks: 863, # CPU blocks: 512\n",
      "INFO 02-07 00:34:07 executor_base.py:113] Maximum concurrency for 4096 tokens per request: 3.37x\n",
      "INFO 02-07 00:34:11 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:23<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-07 00:34:35 model_runner.py:1563] Graph capturing finished in 24 secs, took 0.24 GiB\n",
      "INFO 02-07 00:34:35 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 30.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "from vllm import LLM \n",
    "\n",
    "model = LLM(\n",
    "  model='../optimized-model-merging/llama/llama-mmlu/merged', \n",
    "  tokenizer='meta-llama/Llama-2-7b-hf',\n",
    "  gpu_memory_utilization=0.5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
